import tensorflow as tf
import os
import tensorflow.keras.backend as K

# ==========================================
# Focal Loss (ä¿æŒä¸å˜)
# ==========================================
def categorical_focal_loss(gamma=2.0, alpha=0.25):
    def focal_loss_fixed(y_true, y_pred):
        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
        cross_entropy = -y_true * K.log(y_pred)
        weight = alpha * y_true * K.pow((1 - y_pred), gamma)
        return K.sum(weight * cross_entropy, axis=-1)
    return focal_loss_fixed

# ==========================================
# è®­ç»ƒæµç¨‹ (æ–°å¢žæ–‡ä»¶æ¸…ç†é€»è¾‘)
# ==========================================
def train_model(model, train_ds, val_ds, epochs=10, lr=0.001, save_path='best_model.h5'):
    # ã€æ–°å¢žã€‘: å¦‚æžœæ—§æƒé‡æ–‡ä»¶å­˜åœ¨ï¼Œå…ˆåˆ é™¤ï¼Œé˜²æ­¢ h5py å†™å…¥å†²çª
    if os.path.exists(save_path):
        print(f"âš ï¸ æ£€æµ‹åˆ°æ—§æƒé‡æ–‡ä»¶ {save_path}ï¼Œæ­£åœ¨åˆ é™¤ä»¥é¿å…å†²çª...")
        try:
            os.remove(save_path)
            print("âœ… æ—§æ–‡ä»¶å·²æ¸…é™¤ã€‚")
        except OSError as e:
            print(f"âŒ æ— æ³•åˆ é™¤æ—§æ–‡ä»¶: {e}")

    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    
    # ä½¿ç”¨ Focal Loss
    loss_fn = categorical_focal_loss(gamma=2.0, alpha=0.25)
    
    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])
    
    # Checkpoint å›žè°ƒ
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        save_path, 
        monitor='val_accuracy', 
        save_best_only=True,
        save_weights_only=True,
        verbose=1
    )
    
    early_stop = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss', 
        patience=10, 
        restore_best_weights=True
    )

    print(f"ðŸš€ å¼€å§‹è®­ç»ƒ (M={model.layers[-1].units if hasattr(model, 'layers') and len(model.layers)>0 else '?'}, Loss=Focal)...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        callbacks=[checkpoint, early_stop]
    )
    
    return history